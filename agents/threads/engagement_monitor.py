#!/usr/bin/env python3
"""
X Engagement Monitor ‚Äî Scrapes watchlist accounts for high-performing posts,
drafts reply options, sends to Duncan via Telegram for approval.

Runs 3x daily: 9am, 1pm, 6pm EST
Uses web scraping (no API read access needed)
"""

import json
import subprocess
import sys
from datetime import datetime
from pathlib import Path

WATCHLIST_FILE = Path(__file__).parent / "X_WATCHLIST.md"
CACHE_DIR = Path(__file__).parent / "cache"
CACHE_DIR.mkdir(parents=True, exist_ok=True)

# Priority accounts ‚Äî always check these first
PRIORITY_ACCOUNTS = [
    "AnthropicAI",     # Engage on EVERY launch post
    "AlexHormozi",     # Positioning aligned
    "levelsio",        # 2.8M followers, high visibility
    "gregisenberg",    # Community/startup ideas
    "JustinWelsh",     # Solopreneur crossover
]

# Full watchlist handles (parsed from X_WATCHLIST.md)
ALL_ACCOUNTS = [
    # AI Agents / Claude / OpenClaw
    "AnthropicAI", "alexalbert__", "nickscamara_", "RLanceMartin", "mattshumer_",
    # n8n / Automation
    "max_n8n", "nocodedevs", "zapier",
    # SaaS / Solopreneur
    "levelsio", "dannypostmaa", "marclouvion", "tdinh_me", "gregisenberg",
    # Personal Brand / Creator Economy
    "dickiebush", "SahilBloom", "JamesClear", "AlexHormozi", "LeilaHormozi",
    "dankulture", "JustinWelsh",
    # AI / Tech Thought Leaders
    "swyx", "mattshumer_", "bindureddy",
]


def scrape_recent_posts(handle, max_posts=5):
    """Scrape recent posts from an X account using web_fetch equivalent."""
    # Use nitter or similar public frontend to avoid auth
    # Fallback: direct X profile scrape
    urls_to_try = [
        f"https://xcancel.com/{handle}",
        f"https://nitter.net/{handle}",
    ]
    
    posts = []
    for url in urls_to_try:
        try:
            result = subprocess.run(
                ["python3", "-c", f"""
import urllib.request
import re
import html

req = urllib.request.Request('{url}', headers={{'User-Agent': 'Mozilla/5.0'}})
try:
    with urllib.request.urlopen(req, timeout=10) as resp:
        page = resp.read().decode('utf-8', errors='ignore')
    
    # Extract tweet content from nitter/xcancel HTML
    # Look for tweet-content or timeline-item patterns
    tweet_blocks = re.findall(r'class="tweet-content[^"]*"[^>]*>(.*?)</div>', page, re.DOTALL)
    
    for i, block in enumerate(tweet_blocks[:5]):
        text = re.sub(r'<[^>]+>', '', block).strip()
        text = html.unescape(text)
        if len(text) > 20:
            print(f'TWEET:{{text[:280]}}')
except Exception as e:
    print(f'ERROR:{{e}}')
"""],
                capture_output=True, text=True, timeout=15
            )
            
            for line in result.stdout.strip().split('\n'):
                if line.startswith('TWEET:'):
                    posts.append(line[6:])
            
            if posts:
                break
                
        except Exception:
            continue
    
    return posts


def generate_reply_drafts(handle, tweet_text):
    """Generate 2-3 reply options in Duncan's voice."""
    # These will be generated by the X agent when spawned
    # For now, return the tweet info for the agent to work with
    return {
        "handle": handle,
        "tweet": tweet_text,
        "timestamp": datetime.now().isoformat()
    }


def run_engagement_scan():
    """Main scan ‚Äî check priority accounts, surface engagement opportunities."""
    print(f"üîç Engagement scan starting at {datetime.now().strftime('%I:%M %p EST')}")
    print(f"   Checking {len(PRIORITY_ACCOUNTS)} priority + {len(ALL_ACCOUNTS)} total accounts\n")
    
    opportunities = []
    
    # Priority accounts first
    for handle in PRIORITY_ACCOUNTS:
        print(f"  ‚≠ê @{handle} (priority)...")
        posts = scrape_recent_posts(handle)
        for post in posts:
            opportunities.append({
                "handle": handle,
                "text": post,
                "priority": "high"
            })
    
    # Rest of watchlist
    for handle in ALL_ACCOUNTS:
        if handle in PRIORITY_ACCOUNTS:
            continue
        print(f"  üìã @{handle}...")
        posts = scrape_recent_posts(handle)
        for post in posts:
            opportunities.append({
                "handle": handle,
                "text": post,
                "priority": "normal"
            })
    
    # Save results
    output = {
        "scan_time": datetime.now().isoformat(),
        "total_opportunities": len(opportunities),
        "opportunities": opportunities
    }
    
    output_file = CACHE_DIR / f"engagement_{datetime.now().strftime('%Y%m%d_%H%M')}.json"
    with open(output_file, "w") as f:
        json.dump(output, f, indent=2)
    
    print(f"\n‚úÖ Found {len(opportunities)} engagement opportunities")
    print(f"   Saved to: {output_file}")
    
    # Return top 10 for the agent to draft replies
    return opportunities[:10]


if __name__ == "__main__":
    results = run_engagement_scan()
    if results:
        print(f"\nüèÜ Top engagement opportunities:")
        for i, r in enumerate(results[:10], 1):
            preview = r["text"][:80].replace("\n", " ")
            star = "‚≠ê" if r["priority"] == "high" else "  "
            print(f"  {star} {i}. @{r['handle']}: {preview}...")
